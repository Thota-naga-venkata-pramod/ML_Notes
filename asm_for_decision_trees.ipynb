{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrfX6+Tk1xB6a5qa6Gcx53",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thota-naga-venkata-pramod/ML_Notes/blob/me/asm_for_decision_trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbpY-IM7qkGK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attribute Selection Measures\n",
        "While implementing a Decision tree, the main issue arises that how to select the best attribute for the root node and for sub-nodes. So, to solve such problems there is a technique which is called as Attribute selection measure or ASM. By this measurement, we can easily select the best attribute for the nodes of the tree. There are two popular techniques for ASM, which are:\n",
        "\n",
        "1)Information Gain\n",
        "\n",
        "2)Gini Index\n",
        "\n",
        "1.)Information Gain:\n",
        "\n",
        "Information gain is the measurement of changes in entropy after the segmentation of a dataset based on an attribute.\n",
        "It calculates how much information a feature provides us about a class.\n",
        "According to the value of information gain, we split the node and build the decision tree.\n",
        "\n",
        "A decision tree algorithm always tries to maximize the value of information gain, and a node/attribute having the highest information gain is split first. It can be calculated using the below formula:\n",
        "\n",
        "Information Gain= Entropy(S)- [(Weighted Avg) *Entropy(each feature)]\n",
        "\n",
        "Entropy: Entropy is a metric to measure the impurity in a given attribute. It specifies randomness in data. Entropy can be calculated as:\n",
        "\n",
        "Entropy(s)= -P(yes)log2 P(yes)- P(no) log2 P(no)\n",
        "Where,\n",
        "\n",
        "S= Total number of samples\n",
        "\n",
        "P(yes)= probability of yes\n",
        "\n",
        "P(no)= probability of no\n",
        "\n",
        "#example for information gain:\n",
        "\n",
        "Information gain is a measure used in decision trees to quantify how much a particular feature or attribute contributes to reducing uncertainty in the classification of a dataset. The higher the information gain, the more useful the attribute is in making accurate predictions.\n",
        "\n",
        "Here is an example of how to calculate information gain:\n",
        "\n",
        "Suppose we have a dataset of 20 samples, which we want to classify into two classes, \"positive\" and \"negative\". We have a feature called \"color\", which can take three values: red, blue, or green. We want to calculate the information gain of this feature.\n",
        "\n",
        "Out of the 20 samples, 8 are positive and 12 are negative. The distribution of the \"color\" feature is as follows:\n",
        "\n",
        "6 samples have a \"red\" color: 2 positive, 4 negative\n",
        "\n",
        "8 samples have a \"blue\" color: 4 positive, 4 negative\n",
        "\n",
        "6 samples have a \"green\" color: 2 positive, 4 negative\n",
        "\n",
        "To calculate the information gain of the \"color\" feature, we need to first calculate the entropy of the dataset before and after splitting on the \"color\" feature.\n",
        "\n",
        "The entropy of the dataset before splitting can be calculated as follows:\n",
        "\n",
        "P(positive) = 8/20 = 0.4\n",
        "\n",
        "P(negative) = 12/20 = 0.6\n",
        "\n",
        "Entropy = - (0.4 * log2(0.4) + 0.6 * log2(0.6)) = 0.971\n",
        "\n",
        "Now, we calculate the entropy of the dataset after splitting on the \"color\" feature:\n",
        "\n",
        "For the \"red\" color: P(positive) = 2/6 = 0.33, P(negative) = 4/6 = 0.67\n",
        "\n",
        "Entropy(red) = - (0.33 * log2(0.33) + 0.67 * log2(0.67)) = 0.918\n",
        "\n",
        "For the \"blue\" color: P(positive) = 4/8 = 0.5, P(negative) = 4/8 = 0.5\n",
        "\n",
        "Entropy(blue) = - (0.5 * log2(0.5) + 0.5 * log2(0.5)) = 1\n",
        "\n",
        "For the \"green\" color: P(positive) = 2/6 = 0.33, P(negative) = 4/6 = 0.67\n",
        "\n",
        "Entropy(green) = - (0.33 * log2(0.33) + 0.67 * log2(0.67)) = 0.918\n",
        "\n",
        "Next, we calculate the weighted entropy of the \"color\" feature:\n",
        "\n",
        "Weighted entropy(color) = (6/20) * Entropy(red) + (8/20) * Entropy(blue) + (6/20) * Entropy(green) = 0.946\n",
        "\n",
        "Finally, we can calculate the information gain of the \"color\" feature:\n",
        "\n",
        "Information gain = Entropy(before) - Weighted entropy(color) = 0.971 - 0.946 = 0.025\n",
        "\n",
        "Therefore, the information gain of the \"color\" feature is 0.025, which means that it provides a relatively small amount of information about the classification of the dataset\n",
        "\n",
        "\n",
        "\n",
        "#example for gini index\n",
        "\n",
        "The Gini index is a measure of impurity used in decision tree algorithms to evaluate the quality of a split in the data. The formula for the Gini index is:\n",
        "\n",
        "Gini Index = 1 - (p1^2 + p2^2 + ... + pk^2)\n",
        "\n",
        "where p1, p2, ..., pk are the proportions of each class in the split.\n",
        "\n",
        "Here's an example of how to calculate the Gini index for a binary classification problem with two classes, \"Positive\" and \"Negative\":\n",
        "\n",
        "Suppose we have a dataset of 10 examples with the following labels:\n",
        "\n",
        "Positive, Positive, Positive, Positive, Positive, Negative, Negative, Negative, Negative, Negative\n",
        "\n",
        "To calculate the Gini index for this dataset, we first calculate the proportion of each class:\n",
        "\n",
        "Proportion of Positive class: 5/10 = 0.5\n",
        "\n",
        "Proportion of Negative class: 5/10 = 0.5\n",
        "\n",
        "Then we can plug these proportions into the Gini index formula:\n",
        "\n",
        "Gini Index = 1 - (0.5^2 + 0.5^2) = 0.5\n",
        "\n",
        "This means that the Gini index for this dataset is 0.5, which indicates a relatively high degree of impurity or randomness in the data. If we were to split the data into two subsets based on a certain feature or criterion, we would want to choose the split that minimizes the Gini index to maximize the purity of the resulting subsets."
      ],
      "metadata": {
        "id": "xp-XV7Xnqmcm"
      }
    }
  ]
}