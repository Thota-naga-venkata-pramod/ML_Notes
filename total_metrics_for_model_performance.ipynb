{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvtO1+G1W5BrZqKyn8gyJO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thota-naga-venkata-pramod/ML_Notes/blob/me/total_metrics_for_model_performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-ODpMvcC81c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#total different types of metrics to calculate the model performance\n",
        "\n",
        "There are various metrics that can be used to calculate the performance of a machine learning model, depending on the specific task and the type of model. Here are some common metrics:\n",
        "\n",
        "1)Classification Metrics:\n",
        "\n",
        "Accuracy: The ratio of correct predictions to the total number of predictions.\n",
        "\n",
        "Precision: The ratio of true positives to the total number of positive predictions.\n",
        "\n",
        "Recall: The ratio of true positives to the total number of actual positive samples.\n",
        "\n",
        "F1-Score: The harmonic mean of precision and recall.\n",
        "\n",
        "Area Under the Receiver Operating Characteristic Curve (ROC AUC): A measure of how well the model can distinguish between positive and negative samples.\n",
        "\n",
        "2)Regression Metrics:\n",
        "\n",
        "Mean Squared Error (MSE): The average squared difference between the predicted and actual values.\n",
        "\n",
        "Root Mean Squared Error (RMSE): The square root of MSE.\n",
        "\n",
        "Mean Absolute Error (MAE): The average absolute difference between the predicted and actual values.\n",
        "\n",
        "R-squared: A statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
        "\n",
        "Explained Variance: A measure of how well the model can explain the variance in the dependent variable.\n",
        "\n",
        "3)Clustering Metrics:\n",
        "\n",
        "Silhouette Score: A measure of how well the samples within a cluster are separated from samples in other clusters.\n",
        "\n",
        "Calinski-Harabasz Index: A measure of the ratio of the between-cluster variance to the within-cluster variance.\n",
        "\n",
        "Davies-Bouldin Index: A measure of the average similarity between each cluster and its most similar cluster.\n",
        "\n",
        "4)Recommender System Metrics:\n",
        "\n",
        "Precision: The ratio of the number of relevant recommendations to the total number of recommendations.\n",
        "\n",
        "Recall: The ratio of the number of relevant recommendations to the total number of relevant items.\n",
        "\n",
        "Mean Average Precision (MAP): The average precision for each user over all recommendations.\n",
        "\n",
        "Normalized Discounted Cumulative Gain (NDCG): A measure of the effectiveness of a recommendation system, based on the rank of relevant items in the list of recommendations.\n",
        "\n",
        "\n",
        "These metrics are used to evaluate the performance of the model and to compare different models. It is important to choose the appropriate metric based on the specific task and the type of model being used."
      ],
      "metadata": {
        "id": "45ISbiBJC9kk"
      }
    }
  ]
}